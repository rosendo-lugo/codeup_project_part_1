{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b79439f1-8f0e-4853-ae97-81a5da5e5c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file found and loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>probability_of_churn</th>\n",
       "      <th>prediction_of_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4531-AUZNK</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2296-DKZFP</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9692-TUSXH</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5129-JLPIS</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1273-MTETI</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>1597-FZREH</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>9117-SHLZX</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>8441-SHIPE</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>3511-BFTJW</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>1428-GTBJJ</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1409 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     customer_id  probability_of_churn  prediction_of_churn\n",
       "0     4531-AUZNK                  0.11                    0\n",
       "1     2296-DKZFP                  0.03                    0\n",
       "2     9692-TUSXH                  0.59                    1\n",
       "3     5129-JLPIS                  0.59                    1\n",
       "4     1273-MTETI                  0.83                    1\n",
       "...          ...                   ...                  ...\n",
       "1404  1597-FZREH                  0.41                    0\n",
       "1405  9117-SHLZX                  0.67                    1\n",
       "1406  8441-SHIPE                  0.00                    0\n",
       "1407  3511-BFTJW                  0.03                    0\n",
       "1408  1428-GTBJJ                  0.82                    1\n",
       "\n",
       "[1409 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import acquire as a\n",
    "import prepare as p\n",
    "\n",
    "# Get your telco data\n",
    "telco_df = a.get_telco_data()\n",
    "\n",
    "# Clean the new dataset using the new function called prep_telco\n",
    "telco_df = p.prep_telco(telco_df)\n",
    "\n",
    "# Split your data into train, validate and test\n",
    "train, validate, test = p.split_function(telco_df, 'churn')\n",
    "\n",
    "\n",
    "X_train = train.drop(columns = ['customer_id','payment_type','churn'])\n",
    "X_validate = validate.drop(columns = ['customer_id','payment_type','churn'])\n",
    "X_test = test.drop(columns = ['customer_id','payment_type','churn'])\n",
    "\n",
    "\n",
    "# Set target\n",
    "target = 'churn'\n",
    "\n",
    "# 'y' variables are series\n",
    "y_train = train[target]\n",
    "y_validate = validate[target]\n",
    "y_test = test[target]\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_model.score(X_train, y_train)\n",
    "\n",
    "# load the trained random forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# get the customer IDs from the test data\n",
    "customer_ids = test['customer_id']\n",
    "\n",
    "# create a dataframe with the customer IDs, probabilities, and predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'customer_id': customer_ids,\n",
    "    'probability_of_churn': y_proba,\n",
    "    'prediction_of_churn': y_pred\n",
    "})\n",
    "\n",
    "# write the results to a CSV file\n",
    "results_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "# read the CSV file into a Pandas dataframe\n",
    "predictions_df = pd.read_csv('predictions.csv')\n",
    "\n",
    "# display the dataframe in Jupyter Lab\n",
    "predictions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b0dc31-2fa4-42dc-b04a-7691e06ada79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>probability_of_churn</th>\n",
       "      <th>prediction_of_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4531-AUZNK</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2296-DKZFP</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9692-TUSXH</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5129-JLPIS</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1273-MTETI</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>1597-FZREH</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>9117-SHLZX</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>8441-SHIPE</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>3511-BFTJW</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>1428-GTBJJ</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1409 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     customer_id  probability_of_churn  prediction_of_churn\n",
       "0     4531-AUZNK                  0.17                    0\n",
       "1     2296-DKZFP                  0.02                    0\n",
       "2     9692-TUSXH                  0.52                    1\n",
       "3     5129-JLPIS                  0.52                    1\n",
       "4     1273-MTETI                  0.83                    1\n",
       "...          ...                   ...                  ...\n",
       "1404  1597-FZREH                  0.43                    0\n",
       "1405  9117-SHLZX                  0.66                    1\n",
       "1406  8441-SHIPE                  0.00                    0\n",
       "1407  3511-BFTJW                  0.02                    0\n",
       "1408  1428-GTBJJ                  0.75                    1\n",
       "\n",
       "[1409 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the CSV file into a Pandas dataframe\n",
    "predictions_df = pd.read_csv('predictions.csv')\n",
    "\n",
    "# display the dataframe in Jupyter Lab\n",
    "predictions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a46ec4-9d8e-455e-91c1-eb060d84bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "import numpy as np\n",
    "import acquire\n",
    "import prepare\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def titanic_survived_linear_regression(train, validate, test, list_of_columns, the_C = 1):\n",
    "    logit1 = LogisticRegression(C =the_C, max_iter=1000)\n",
    "    logit1\n",
    "    train\n",
    "    X_train = train.loc[: , list_of_columns]\n",
    "    y_train = train.survived\n",
    "    X_validate = validate.loc[: , list_of_columns]\n",
    "    y_validate = validate.survived\n",
    "    X_test = test.loc[: , list_of_columns]\n",
    "    y_test = test.survived\n",
    "    logit1.fit(X_train, y_train)\n",
    "    train_score = logit1.score(X_train, y_train)\n",
    "    validate_score = logit1.score(X_validate, y_validate)\n",
    "    test_score = logit1.score(X_test, y_test)\n",
    "    return train_score, validate_score, test_score\n",
    "\n",
    "dict_for_dataframe = {}\n",
    "df = acquire.get_titanic_data()\n",
    "df = prepare.prep_titanic(df)\n",
    "df['age'] = df['age'].replace(np.nan,0)\n",
    "train, validate, test = prepare.split_data(df, 'survived')\n",
    "list_of_columns = train.columns\n",
    "the_c_list = [.01, .1, 1, 10, 100, 1000]\n",
    "train_list = []\n",
    "validate_list = []\n",
    "features= []\n",
    "\n",
    "for c_value in the_c_list:\n",
    "    for num_cols in range(2, len(list_of_columns)+1):\n",
    "        for i in itertools.combinations(list_of_columns, num_cols):\n",
    "            print(i)\n",
    "            train_score, validate_score, test_score = titanic_survived_linear_regression(train, validate, test, i, the_C = c_value)\n",
    "            train_list.append(train_score)\n",
    "            validate_list.append(validate_score)\n",
    "            features.append(i)\n",
    "the_dataframe = pd.DataFrame({'train':train_list,\n",
    "             'validate':validate_list,\n",
    "             'features':features})\n",
    "\n",
    "the_dataframe['difference'] = abs(the_dataframe.train - the_dataframe.validate)\n",
    "the_dataframe = the_dataframe.sort_values(by='difference')\n",
    "\n",
    "print(the_dataframe[the_dataframe.difference > 0])\n",
    "plt.plot(range(0, len(the_dataframe.train)), the_dataframe.train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d8c82a-acfb-4a7c-9a54-962b2670b76f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07315c59-c579-47cd-bdc9-37a607792146",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.drop(['embarked', 'alone', 'sex_male', 'embarked_Q', 'embarked_S'], axis=1, inplace=True)\n",
    "titanic_df['sex'] = titanic_df['sex'].map({'male': 0, 'female': 1})\n",
    "titanic_df['age'].fillna(titanic_df['age'].median(), inplace=True)\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(titanic_df.drop('survived', axis=1), titanic_df['survived'], test_size=0.2, random_state=42)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "hyperparameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "# Define the logistic regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Define the grid search cross-validation\n",
    "grid_search = GridSearchCV(logreg, hyperparameters, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding accuracy score\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score on validation data: \", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the model on the testing data using the best hyperparameters\n",
    "logreg_best = LogisticRegression(max_iter=1000, C=grid_search.best_params_['C'])\n",
    "logreg_best.fit(X_train, y_train)\n",
    "y_pred = logreg_best.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy score on testing data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8703116-2242-4e48-8ba9-eb7ba3d8b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "def get_predictions_df(model, X_test):\n",
    "    '''\n",
    "    This function takes a trained model and a dataframe X_test as input,\n",
    "    and returns a dataframe containing the predictions for each customer in X_test.\n",
    "    The dataframe has 3 columns: customer_id, probability of churn, and prediction of churn.\n",
    "    '''\n",
    "\n",
    "    # Load the best performing model\n",
    "    model = joblib.load(\"best_model.pkl\")\n",
    "\n",
    "    # Make predictions on X_test\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Create a dataframe with customer_id, probability of churn, and prediction of churn\n",
    "    customer_id = X_test[\"customer_id\"]\n",
    "    predictions = pd.DataFrame({\n",
    "        \"customer_id\": customer_id,\n",
    "        \"probability_of_churn\": y_proba,\n",
    "        \"prediction_of_churn\": y_pred\n",
    "    })\n",
    "\n",
    "    # Write the predictions dataframe to a CSV file\n",
    "    predictions.to_csv(\"predictions.csv\", index=False)\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e29d9c4-5907-49f9-ab07-22d56858b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_predictions(model, X_test, customer_ids, output_path):\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Create a DataFrame with the customer IDs, predicted probabilities, and predicted classes\n",
    "    customer_id = X_test[\"customer_id\"]    \n",
    "    predictions_df = pd.DataFrame({'customer_id': customer_id, \n",
    "                                'probability_of_churn': y_proba, \n",
    "                                'prediction_of_churn': y_pred})\n",
    "\n",
    "    # Convert the predicted classes to binary values (1 = churn, 0 = not churn)\n",
    "    predictions_df['prediction_of_churn'] = predictions_df['prediction_of_churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "\n",
    "    # Write the predictions dataframe to a CSV file\n",
    "    predictions_df.to_csv(\"predictions.csv\", index=False)\n",
    "    \n",
    "    return predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
