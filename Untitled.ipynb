{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a46ec4-9d8e-455e-91c1-eb060d84bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "import numpy as np\n",
    "import acquire\n",
    "import prepare\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def titanic_survived_linear_regression(train, validate, test, list_of_columns, the_C = 1):\n",
    "    logit1 = LogisticRegression(C =the_C, max_iter=1000)\n",
    "    logit1\n",
    "    train\n",
    "    X_train = train.loc[: , list_of_columns]\n",
    "    y_train = train.survived\n",
    "    X_validate = validate.loc[: , list_of_columns]\n",
    "    y_validate = validate.survived\n",
    "    X_test = test.loc[: , list_of_columns]\n",
    "    y_test = test.survived\n",
    "    logit1.fit(X_train, y_train)\n",
    "    train_score = logit1.score(X_train, y_train)\n",
    "    validate_score = logit1.score(X_validate, y_validate)\n",
    "    test_score = logit1.score(X_test, y_test)\n",
    "    return train_score, validate_score, test_score\n",
    "\n",
    "dict_for_dataframe = {}\n",
    "df = acquire.get_titanic_data()\n",
    "df = prepare.prep_titanic(df)\n",
    "df['age'] = df['age'].replace(np.nan,0)\n",
    "train, validate, test = prepare.split_data(df, 'survived')\n",
    "list_of_columns = train.columns\n",
    "the_c_list = [.01, .1, 1, 10, 100, 1000]\n",
    "train_list = []\n",
    "validate_list = []\n",
    "features= []\n",
    "\n",
    "for c_value in the_c_list:\n",
    "    for num_cols in range(2, len(list_of_columns)+1):\n",
    "        for i in itertools.combinations(list_of_columns, num_cols):\n",
    "            print(i)\n",
    "            train_score, validate_score, test_score = titanic_survived_linear_regression(train, validate, test, i, the_C = c_value)\n",
    "            train_list.append(train_score)\n",
    "            validate_list.append(validate_score)\n",
    "            features.append(i)\n",
    "the_dataframe = pd.DataFrame({'train':train_list,\n",
    "             'validate':validate_list,\n",
    "             'features':features})\n",
    "\n",
    "the_dataframe['difference'] = abs(the_dataframe.train - the_dataframe.validate)\n",
    "the_dataframe = the_dataframe.sort_values(by='difference')\n",
    "\n",
    "print(the_dataframe[the_dataframe.difference > 0])\n",
    "plt.plot(range(0, len(the_dataframe.train)), the_dataframe.train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07315c59-c579-47cd-bdc9-37a607792146",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.drop(['embarked', 'alone', 'sex_male', 'embarked_Q', 'embarked_S'], axis=1, inplace=True)\n",
    "titanic_df['sex'] = titanic_df['sex'].map({'male': 0, 'female': 1})\n",
    "titanic_df['age'].fillna(titanic_df['age'].median(), inplace=True)\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(titanic_df.drop('survived', axis=1), titanic_df['survived'], test_size=0.2, random_state=42)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "hyperparameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "# Define the logistic regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Define the grid search cross-validation\n",
    "grid_search = GridSearchCV(logreg, hyperparameters, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding accuracy score\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score on validation data: \", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the model on the testing data using the best hyperparameters\n",
    "logreg_best = LogisticRegression(max_iter=1000, C=grid_search.best_params_['C'])\n",
    "logreg_best.fit(X_train, y_train)\n",
    "y_pred = logreg_best.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy score on testing data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8703116-2242-4e48-8ba9-eb7ba3d8b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "def get_predictions_df(model, X_test):\n",
    "    '''\n",
    "    This function takes a trained model and a dataframe X_test as input,\n",
    "    and returns a dataframe containing the predictions for each customer in X_test.\n",
    "    The dataframe has 3 columns: customer_id, probability of churn, and prediction of churn.\n",
    "    '''\n",
    "\n",
    "    # Load the best performing model\n",
    "    model = joblib.load(\"best_model.pkl\")\n",
    "\n",
    "    # Make predictions on X_test\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Create a dataframe with customer_id, probability of churn, and prediction of churn\n",
    "    customer_id = X_test[\"customer_id\"]\n",
    "    predictions = pd.DataFrame({\n",
    "        \"customer_id\": customer_id,\n",
    "        \"probability_of_churn\": y_proba,\n",
    "        \"prediction_of_churn\": y_pred\n",
    "    })\n",
    "\n",
    "    # Write the predictions dataframe to a CSV file\n",
    "    predictions.to_csv(\"predictions.csv\", index=False)\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e29d9c4-5907-49f9-ab07-22d56858b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_predictions(model, X_test, customer_ids, output_path):\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Create a DataFrame with the customer IDs, predicted probabilities, and predicted classes\n",
    "    customer_id = X_test[\"customer_id\"]    \n",
    "    predictions_df = pd.DataFrame({'customer_id': customer_id, \n",
    "                                'probability_of_churn': y_proba, \n",
    "                                'prediction_of_churn': y_pred})\n",
    "\n",
    "    # Convert the predicted classes to binary values (1 = churn, 0 = not churn)\n",
    "    predictions_df['prediction_of_churn'] = predictions_df['prediction_of_churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "\n",
    "    # Write the predictions dataframe to a CSV file\n",
    "    predictions_df.to_csv(\"predictions.csv\", index=False)\n",
    "    \n",
    "    return predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
